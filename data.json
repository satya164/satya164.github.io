[{"id":"automated-release-on-github-actions","title":"Automated Release on GitHub Actions","description":"Releasing an npm package usually has a lot of steps. This is a guide to setting up automated releases for npm packages on GitHub Actions using release-it.","content":"There are many tools available to automate the release process for npm packages, such as lerna-lite for monorepos, Release It! for single packages etc. They can be used to automatically bump the version and generate changelog based on the commit messages using Conventional Commits, create a GitHub release, create a git tag, and publish the package to npm etc.\n\nGenerally, the release process is triggered manually by running a command on local machine. But automating the release process on GitHub Actions can be more convenient.\n\nThis guide documents how to configure GitHub Actions to automatically release npm packages on every commit using release-it. However, these steps can be adapted to other similar tools as well.\n\nStep 1\n\nInstall release-it and @release-it/conventional-changelog as dev dependencies:\n\nConfigure release-it in the package.json file:\n\nStep 2\n\nCreate a NPM token with publish access. You can create one at https://www.npmjs.com/settings/\\[username]/tokens (replace \\[username] with your username):\n\nClick on \"Generate New Token\" and select \"Granular Access Token\"\n\nProvide a token name and expiration date\n\nUnder \"Packages and scopes\", select \"Read and write\" for permissions\n\nThen select \"Only select packages and scopes\" and select the package you want to publish\n\nClick \"Generate token\" and copy the token\n\nThen the token needs to be added as a secret in the GitHub repository:\n\nGo to the repository and click on \"Settings\"\n\nClick on \"Secrets and variables\" and choose \"Actions\"\n\nClick \"New repository secret\" and add the token as NPM\\_PUBLISH\\_TOKEN\n\nClick on \"Add secret\" to save the token\n\nThis token will be used to authenticate with NPM to publish the package.\n\nStep 3\n\nCreate a GitHub personal access token. You can create one at github.com/settings/personal-access-tokens/new:\n\nProvide name, description, and expiration date as per your preference\n\nUnder \"Repository access\", select \"Only select repositories\" and choose the repositories you want to set up the release workflow for\n\nUnder \"Permissions\", expand \"Repository permissions\" and set \"Contents\" to \"Access: Read & write\"\n\nClick \"Generate token\" and copy the generated token\n\nAlternatively, you can create a classic token with the repo scope under Developer settings in your profile settings. However, it is highly recommended to use granular access tokens with the least required permissions.\n\nThen the token needs to be added as a secret in the GitHub repository:\n\nGo to the repository and click on \"Settings\"\n\nClick on \"Secrets and variables\" and choose \"Actions\"\n\nClick \"New repository secret\" and add the token as PERSONAL\\_ACCESS\\_TOKEN\n\nClick on \"Add secret\" to save the token\n\nA personal access token is necessary to be able to push the changes back to the repository if the release branch is protected. The user associated with the token needs to have admin access to the repository and be able to bypass branch protection rules.\n\n\\[!WARNING]\nOther collaborators on the repo can push actions that use this token and push commits acting as the user associated with the token.\n\nIf there are no branch protection rules in the repository, then the GITHUB\\_TOKEN secret can be used instead of a personal access token.\n\nStep 4\n\nCreate a GitHub Actions workflow file in .github/workflows/release.yml with the following contents:\n\nThere are 2 important things to note in this workflow:\n\nThe workflow runs on the workflow\\_run event. This event is triggered when another workflow is run. In this case, the CI workflow is run on every commit to the main branch. The release workflow is triggered when the CI workflow is completed. You may need to change the name according to the name of the workflow that runs tests, linting, etc. in your repository.\n\nThere are 2 jobs in the workflow. The first job checks if the commit message is a release commit. If it is, then the second job is skipped. This is to prevent an infinite loop of releases. The second job runs release-it to publish the package.\n\nAdditionally, setting NPM\\_CONFIG\\_PROVENANCE to true will generate a provenance statement when publishing the package. This lets others verify where and how your package was built. This also needs the id-token: write permission in the permissions section of the job.\n\nAfter configuring, this workflow automatically publishes a new version of the package on every commit to the main branch after the CI workflow is successful.\n\nRelease workflow\n\nInstead of publishing on every commit, an alternative way could be to have the release workflow configured, and run the workflow manually from the Actions tab in the repository when a new release is needed. This can be done by using the workflow\\_dispatch event to the on section:\n\nSee the GitHub documentation for Manually running a workflow for more details.\n"},{"id":"how-to-write-a-minimal-code-editor","title":"How to write a minimal code editor","description":"Writing a code editor is a complex task. But what if there was an alternative approach for simpler use cases?","content":"Code editors are complex. They do a lot of things to emulate the same experience as normal textarea while providing a ton of additional features such as indenting the code, autocomplete, annotations, multiple cursors etc. In this article, we'll do none of these things.\n\nWell, that's disappointing. I hear you say. Wait, don't close the page. I still have something for you.\n\nSometimes, all you want is a basic editor to embed some editable code on a page. You don't need all the features of the complex editors. All you care about is that it should be lightweight and should load fast. We'll discuss an approach to achieve just that.\n\nHow do code editors work?\n\nTypically, code editors such as CodeMirror, Ace, Monaco etc. use a hidden textarea to detect what you type and other operations. Then they reflect those changes in the DOM so it appears like you're directly editing it. The caret you see is fake and is just a div pretending to be a caret. There is usually a lot more going on, and some editors may take a different approach. But that's the gist of it.\n\nAnother approach is to use contentEditable div which allows you to make any element editable. react-live uses this approach. But contentEditable can often be tricky to work with.\n\nBut we are here to build a minimal code editor, so we'll take a different route that's easier to implement.\n\nUsing a hidden textarea\n\nWhile we're not going to follow the same approach as other editors, a hidden textarea is still going to be a part of our solution.\n\nThe idea is that we use a normal textarea for our editor, highlight the code with prismjs and align it on top of the textarea. Whenever the textarea's value changes, we update the highlighted code. Then we disable interactions with the highlighted code by using pointer-events: none and whenever you interact with the editor, you actually interact with the textarea.\n\nI got this idea from a library (kueblc/LDT) around 5 years ago.\n\nHere is a demo editor with this approach:\n\nWe are doing the following things here:\n\nWe use a textarea with the code and a pre element to display the highlighted code.\n\nThe text is kept in sync by updating the pre element whenever the textarea's value changes.\n\nThe pre has pointer-events: none so that you can't interact with it.\n\nAdditional styles are applied to make the textarea and pre look the same with same font, line height etc.\n\nThe text in the textarea is hidden by -webkit-text-fill-color: transparent (it also works on non-webkit browsers such as Firefox).\n\nThe syntax highlighting is done by prismjs.\n\nSo now we have a minimal code editor which is super lightweight and good enough for simple use cases.\n\nWhat's missing?\n\nA good code editor is not only text with syntax highlighting. There are still a few more things we need to do to make it usable.\n\nIndentation\n\nA large part of writing code is indenting it, and a simple text area doesn't provide this feature.\n\nTo implement indentation support with tab, we will need to listen to keydown events from the textarea and check if tab key was pressed. Then we can prevent the default behaviour and insert the tab character programmatically.\n\nTo handle tabs properly (inserting tab character, indenting and unindenting a selected text while maintaining proper selection etc.), a fair bit of code is required, so I'm not going to discuss it here.\n\nUndo/redo\n\nWhen updating the textarea programmatically, the undo stack gets lost. We can use document.execCommand instead to insert the text, which will preserve the undo stack. However, it's a deprecated feature and also doesn't work on Firefox (bug 1220696), so it's not a feasible solution.\n\nThe only option I know at the time of writing this post is to implement a custom undo manager and maintain our own undo stack. Then we can listen to keydown events for undo/redo shortcuts, and apply the changes ourselves instead of relying on the browser.\n\nSwitching focus\n\nSince we are intercepting tab key, the default behaviour of switching focus is now gone, which is not good for accessibility. The best we can do here is to provide a keybinding which allows toggling this behaviour.\n\nLimitations\n\nThere are limitations of this approach to be aware of:\n\nThe syntax highlighted code cannot have different font family, font weight, font style, line height etc. for its content. Since the editor works by aligning the highlighted code over a textarea, changing anything that affects the layout can misalign it.\n\nThe custom undo stack is incompatible with undo/redo items browser's context menu.\n\nSince the editor works by syntax highlighting the all of the code and inserting it DOM, unlike other editors which tokenize and highlight only parts of the code, the editor is not optimized for performance and large documents can inversely affect the typing speed.\n\nBecause of these limitations, this approach is not suitable for full featured code editors. But it's good enough for basic use cases.\n\nWrapping up\n\nA while ago I made this idea into a package: react-simple-code-editor. It implements the indentation, undo/redo and focus switching features discussed above. The syntax highlighting can be done by any third party library (such as Prism). You can try the demo here.\n\nThis component is used in snack.expo.dev to provide a minimal code editor for embedding code snippets, as well as a replacement for the default editor on slower connections.\n"},{"id":"stacking-context-in-css","title":"Stacking context - the hidden layers in CSS","description":"How do you position an element on top of another element in CSS? Just use z-index, right? Right?? Turns out, there is a bit more to it.","content":"We often reach for z-index when we want to position an element on top of another element. But it doesn't always work as expected. Sometimes even using a ridiculously high value like z-index: 9999 doesn't seem to do anything. So what stops z-index from working as expected? The answer lies in stacking context.\n\nStacking context refers to how elements are stacked or layered along an imaginary z-axis. Kind of like layers in Photoshop. These layers are stacked relative to each other.\n\nLayers\n\nWe can customize the order of stacking within a stacking context using z-index property. The higher the z-index value, the higher the element will be stacked and it'll be visible on top of other elements. Adding z-index to an element works because the root element (html) creates a stacking context, and all the elements are stacked within it by default.\n\nNested stacking contexts\n\nStacking contexts can be nested. So when we add a z-index to an element, we are not only changing the stacking order of that element but also creating a new stacking context. This means that elements within that element will be stacked relative to that element, and not the root element.\n\nIn the following demo, we have 3 elements, each with a different z-index value.\n\nThe yellow box has a z-index of 3 - which is the highest\n\nThe blue box has a z-index of 2 - which is in the middle\n\nThe red box has a z-index of 1 - which is the lowest\n\nTypically, we may expect the elements to be positioned so that the red box is at the bottom, the blue box is in the middle, and the yellow box is on top. But in the demo, we see that the yellow box is at the bottom even though it has the highest z-index.\n\nThis is because the yellow box is inside a div that has z-index of 0 - which creates a new stacking context. So it is stacked within that div and cannot be stacked any higher or lower. The elements inside this stacking context are stacked independently from the elements in the parent stacking context.\n\nIf you remove the z-index from the parent div, it'll no longer create a new stacking context and the yellow box will be on top.\n\nWhat creates a new stacking context?\n\nSo far we've seen that adding a z-index creates a new stacking context. So far it doesn't seem too complicated. But there are many more ways a stacking context can be created.\n\nFor example, let's consider this demo:\n\nHere, we don't have any z-index on the parent div, but the yellow box is still at the bottom. This is because the parent div has an opacity of 0.99 - which creates a new stacking context. Try changing the opacity to 1 and notice how the yellow box is now on top.\n\nThis is quite unexpected. But yes, any opacity value lower than 1 will create a new stacking context. Similarly, other properties such as transform, filter etc. also create a new stacking context.\n\nYou can see a full list of scenarios that create a new stacking context on MDN.\n\nSince stacking contexts can be created in so many ways, often unintentionally as we're styling our elements, this can lead to unexpected results when using z-index. So being aware of this can help us understand why z-index doesn't work sometimes and how to fix it.\n"},{"id":"publishing-dual-module-esm-libraries","title":"Publishing dual module ESM libraries","description":"Dual module libraries containing both ESM and CommonJS modules can be tricky. Here's what I learned.","content":"Recently I've been looking into moving to ES modules for my libraries and wanted to do it in a way that doesn't break existing users. But it's tricky - and took a lot of trial and error. In this post, I'll share my findings.\n\nESM and CommonJS\n\nES modules (ESM) are the official standard module system in JavaScript. They are supported in modern browsers and Node.js, as well as by most bundlers. They are defined by the import and export keywords:\n\nCommonJS modules (CJS) are the module system that has been traditionally used in Node.js, but also adopted by most tools and bundlers. They are defined by the require function and the module.exports object:\n\nSo depending on the environment, we'd need to use the appropriate module system.\n\nWhy dual module libraries?\n\nWhen publishing a library written with ES modules, we may want to provide both ESM and CommonJS modules. This can be necessary for a couple of reasons:\n\nSome tools or environments may not support ES modules yet, so providing a CommonJS version can be useful.\n\nNode.js code written with CommonJS modules can only import ESM code asynchronously with dynamic import syntax (synchronous require is available for Node.js 22 behind a flag: --experimental-require-module as of August 2024), so providing a CommonJS version is necessary for synchronous usage.\n\nEssentially, providing both ESM and CommonJS modules is a way to move to ESM without a breaking change.\n\nAnd of course, if you don't need to support CommonJS environments, you can publish your library as an ESM-only package which is much simpler.\n\n\\[!NOTE]\nSynchronous require for ESM is now unflagged in Node.js 22 and in the process of being backported to Node.js 20. So if you're targeting Node.js 20 or later, consumers still using CommonJS can seamlessly use your library without needing to publish a dual module setup.\n\nEntry points in package.json\n\nThe package.json can contain various fields that point to the file that should be loaded when the package is imported or required.\n\nThe most common fields are:\n\nmain: This field has been supported in Node.js for a long time and points to the CommonJS entry point. This is supported by the majority of tools and bundlers.\n\nmodule: This field is used by bundlers like webpack or Rollup to load the ESM entry point. This field is not supported in Node.js.\n\nexports: This field is a newer addition to the package.json specification and allows specifying multiple entry points for different environments conditionally.\n\nIn addition, if you're writing client-side code, you may have come across the following fields:\n\nbrowser: This field is used to specify a different entry point for client-side code for the browser. This field is used by bundlers such as webpack and Rollup.\n\nreact-native: This field is used to specify a different entry point for React Native environments. This field is supported by bundlers like Metro.\n\nWhen writing dual modules, we'd want to use the exports field to specify both ESM and CommonJS entry points, while also providing the main and module fields for backwards compatibility.\n\nThe exports field in package.json\n\nThe exports field in package.json allows specifying multiple entry points for different environments conditionally. It can be used to specify ESM and CommonJS entry points for dual module libraries.\n\nHere we'll cover the 2 most common cases. You can find more information in the official Node.js documentation for entry points.\n\nConditional exports\n\nThe exports field specifies conditions for different environments. Think of it like an if-else statement - the module resolution goes through each of the conditions one by one and uses the first one that matches.\n\nA basic example of exports field would be:\n\nHere, the condition is as follows:\n\nWhen the package is imported, the import condition is used\n\nWhen the package is required, the require condition is used\n\nIn addition, the conditions can also specify a default for the fallback, which is used if none of the conditions match:\n\nIn the above example, the condition is as follows:\n\nWhen the package is imported in a browser environment, the browser condition is used\n\nWhen the package is imported in a React Native environment, the react-native condition is used\n\nIf neither of the conditions matches, the default condition is used\n\nWhat conditions are available depends on the environment and the tooling used for module resolution.\n\nThe order of the conditions is important. Multiple conditions may match, e.g. if we have the conditions node and require - both would match when the package is required in Node.js. In this case, the first condition that matches is used:\n\nIn the above example, the require condition would always match first, so the node condition would never be used. The correct order would be:\n\nIt is recommended to use the most specific conditions first, and the most general conditions last.\n\nThe conditions can also be nested. For example:\n\nThis can be useful if you want to have more specific conditions for certain environments.\n\nSubpath exports\n\nWhen the exports field is defined, it's no longer possible import a subpath of the package directly. For example, if we have the following exports field:\n\nWe can't import or require a subpath of the package directly:\n\nSimilar to how . points to the main entry point, subpaths can also be specified in the exports field:\n\nThis will now allow importing or requiring my-package/foo.js.\n\nAmbiguity in ESM and CommonJS\n\nCommonJS and ESM have different semantics, with different sets of features and limitations. This means that we need to be explicit about which module system we're using.\n\nThere are a few ways to specify the module system:\n\nThe type field in package.json\n\nThe type field in package.json can be used to specify the module system used by the package in Node.js. The value can be either module for ESM or commonjs for CommonJS:\n\nWhen the type field is set to module, all .js files in the package are treated as ESM files. When the type field is set to commonjs, all .js files are treated as CommonJS files.\n\nBy default, the type field is assumed to be commonjs if not specified.\n\nFile extension\n\nThe file extension can also be used to specify the module system in Node.js:\n\n.mjs files are treated as ESM files.\n\n.cjs files are treated as CommonJS files.\n\nRegardless of the type field, files with the .mjs extension are always treated as ESM files, and files with the .cjs extension are always treated as CommonJS files in Node.js.\n\nScript tag type attribute\n\nIn the browser, the module system is determined by the type attribute in the script tag:\n\nWhen the type attribute is set to module, the file is treated as an ESM file. When the type attribute is not specified or set to text/javascript, the file is treated as a CommonJS file.\n\nUnlike Node.js, the file extension does not determine the module system in the browser, and browsers don't read the package.json file.\n\nExplicit file extensions\n\nUnlike CommonJS modules, ES modules in Node.js require explicit file extensions in import/export statements:\n\nWhile explicit file extensions are not required in browsers - as the import specifier is a URL and the server can be configured to serve the correct file, it can still be simpler to use file extensions to avoid additional logic on the server.\n\nApproaches\n\nThere are 2 main approaches to publishing dual module libraries:\n\nES module wrapper with CommonJS code\n\nThis is the simplest approach. You write your library in CommonJS and create an ESM wrapper around it. The ESM wrapper imports the CommonJS code and re-exports it:\n\nThen in your package.json, you'd specify the exports field to point to the ESM wrapper for ESM environments and the CommonJS module for CommonJS environments, as well as fallbacks with main and module fields:\n\nThis avoids the need to refactor your code to ESM.\n\nHowever, this approach has a few downsides:\n\nThe ESM wrapper needs to be maintained manually, so it can be error-prone.\n\nSince our code is still written in CommonJS, we don't get the benefits of ESM like tree-shaking with bundlers.\n\nSeparate ESM and CommonJS builds\n\nThis approach involves writing your library in ESM and CommonJS separately - or more commonly, authoring in ESM and using tooling to generate the CommonJS build.\n\nThen in your package.json, you'd specify the exports field to point to the ES module for ESM environments and the CommonJS module for CommonJS environments, as well as fallbacks with main and module fields:\n\nThis approach has the benefit of allowing you to write your code in ESM and get the benefits of ESM like tree-shaking with bundlers. But it also increases complexity in the build process.\n\nWhen following this approach, you may encounter a few issues:\n\nDual package hazard\n\nWith this approach, the ESM and CommonJS versions of the package are treated as separate modules by Node.js as they are different files, leading to potential issues if the package is both imported and required in the same runtime environment.\n\nIf the package relies on any state that can cause issues if 2 separate instances are loaded, it's necessary to isolate the state into a separate CommonJS module that can be shared between the ESM and CommonJS builds.\n\nThis is not an issue if it's safe to have 2 separate instances of the package loaded in the same environment, which is often the case for most libraries.\n\nMismatched module type\n\nThe import and require conditions only tell Node.js which file to load when the package is imported or required, but they don't say which module system is used in the file. So it's possible to specify an ES module in the require condition and a CommonJS module in the import condition, which may not work as expected.\n\nTo avoid this footgun, you can do any of the following:\n\nAvoid using .js files and use the .cjs and .mjs file extensions to specify the module system.\n\nSpecify type: 'commonjs' in package.json to treat all .js files as CommonJS and use .mjs files for the ESM build.\n\nSpecify type: 'module' in package.json to treat all .js files as ESM and use .cjs files for the CommonJS build.\n\nLack of support for .mjs or .cjs\n\nSince we aim to support older environments that don't support the new ES module system, they may not recognize the .mjs or .cjs file extensions. Most modern tools and bundlers support the .mjs and .cjs file extensions, but they might also differ in how they treat these files. For example, Vite allows importing .mjs files without explicit file extensions, but Metro doesn't.\n\nOne way to avoid this issue is to use the .js file extension for both ESM and CommonJS files. But how do we specify the module system in this case? We can't use the type field in the project's package.json as it applies to all .js files. But we can create package.json files with a type field in each of the build folders:\n\nAnd then in the main package.json, we point to the respective .js files:\n\nLack of support for Platform-specific extension\n\nWhen writing cross-platform code, such as code that supports React Native, we often use platform-specific extensions such as .android.js, .ios.js, .native.js, etc. However, this doesn't work with the explicit file extension requirement in ESM.\n\nFor example, let's say we have 2 files: foo.android.js and foo.js, and an import statement: import foo from './foo'. Normally the bundler would resolve foo.android.js for Android and foo.js for other platforms. But in ESM, the file extension is required, so the import statement would need to be import foo from './foo.js' - which would break the platform-specific resolution as now the bundler would always resolve foo.js.\n\nAlternative approaches to handle this would be to:\n\nUse a separate CommonJS build that contains the platform-specific files, and can import them without specifying the file extension.\n\nInstead of separate files, use a single file with platform-specific logic conditionally executed based on the platform.\n\nOmit the extension from the import statement - while this won't work on Node.js, bundlers such as Metro, Webpack etc. still support ESM without file extensions.\n\nTypeScript\n\nConfiguration\n\nWhen writing ES modules in TypeScript, it's necessary to configure the module and moduleResolution options in tsconfig.json:\n\nWhen the module option is set to NodeNext (or Node16), TypeScript generates ES module syntax in the output. It also requires file extensions in import statements.\n\nFile extensions in import statements\n\nWhen using TypeScript with ES modules, it's necessary to specify the .js file extensions in import statements:\n\nIn this case, the authored file module.ts would have the .ts extension and not .js, however, we need to specify the .js extension in the import statement to match the output file extension.\n\nTypeScript has an option: allowImportingTsExtensions: true to write ./module.ts instead of ./module.js in the import statement. It's also possible to specify moduleResolution: 'Bundler' to allow omitting the file extension in the import statement. However, TypeScript compiler doesn't rewrite the imports to add the correct file extension, so unless they are added by another tool, the imports will fail at runtime.\n\nTypeScript also supports .mts and .cts file extensions. When these extensions are used in combination with module: 'NodeNext', TypeScript generates ESM and CommonJS output accordingly. It can be useful if you explicitly want to specify a module system for a file. However, for our setup where we always author ESM and generate 2 builds for ESM and CommonJS, using these extensions will complicate the build process.\n\nDefault exports\n\nDefault exports in TypeScript can be problematic. Let's consider the following code:\n\nThis code works in ESM with the following:\n\nHowever, when the code is compiled to CommonJS, the default export is converted to an object with a default property:\n\nSo now the import statement would need to be:\n\nThis can be problematic when writing dual modules, as what we import in ESM is different from what we import in CommonJS. Ideally, we want the following CommonJS output:\n\nWe can get this output if we change the source code to the following:\n\nHowever, this is not compatible with compiling to ESM.\n\nTo workaround this issue, there are a 2 options:\n\nUse named exports instead of default exports.\n\nAdd a wrapper for the CommonJS build that re-exports the default export with export =.\n\nTypes in the exports field\n\nWhen publishing dual module libraries, it's necessary to provide separate declaration files for both ESM and CommonJS modules. Declaration files can be specified using the types condition in the exports field:\n\nIn the above case, either the file extension (.d.mts or .d.cts) or a package.json in each build folder containing a type field can be used to specify the module system.\n\nIf we don't have separate declaration files for each module system, it will cause issues:\n\nCommonJS types\n\nIf the library's package.json has no type field or type: 'commonjs', the types will be treated as CommonJS types, i.e. types representing a CommonJS build.\n\nThis will result in incorrect types when the library is imported with import, as the ESM build will get imported which doesn't match the types. Consider the following example:\n\nNow, when the library is imported, TypeScript will allow the following:\n\nThis would've worked if the CommonJS build was being used, however, since the ESM build is being used, the above code won't work during runtime as the library doesn't have a default export. The correct code would be:\n\nESM types\n\nIf the library's package.json has type: 'module', the types will be treated as ESM types, i.e. types representing an ESM build.\n\nIn this case, TypeScript will produce an error when the library is imported with require or with import in a project with CommonJS output, as the CommonJS build will get imported which doesn't match the types:\n\nThis happens because it's currently not possible to import ESM modules synchronously from CommonJS modules in Node.js. However, we're using the CommonJS build during runtime, so this error is incorrect.\n\nUseful tools\n\nWriting dual module libraries can be complex, so here are some tools that can help:\n\narethetypeswrong - A tool to check if the types in the exports field of a package.json are correct.\n\ntshy - A build tool that handles generating dual module builds and declaration files with minimal configuration.\n\nreact-native-builder-bob - A build tool for React Native libraries that can rewrite imports to add the correct file extension for ESM compatibility.\n\nWrapping up\n\nWriting dual module libraries has a lot of nuances and can be tricky. And some of the problems can take a lot of work to solve.\n\nHere are my recommendations:\n\nUse .js extensions for both ESM and CommonJS builds with a package.json file in each build folder to specify the module system (type: 'module' for ESM and type: 'commonjs' for CommonJS).\n\nUse .ts extension for TypeScript files instead of .mts or .cts so that we get .js output files\n\nUse .js extension in the import statements when importing TypeScript files, unless another tool rewrites the imports to add the correct file extension.\n\nIf you need to support platform-specific extensions, don't use .js extension for imports to avoid breaking platform-specific resolution.\n\nUse named exports instead of default exports to avoid inconsistent output between ESM and CommonJS builds when compiling with TypeScript or Babel.\n\nBe mindful of the order of conditions in the exports field and use the most specific conditions first.\n\nUse tools like tshy or react-native-builder-bob to simplify the build process instead of maintaining it manually.\n\nA typical package.json for such a setup would look like this:\n\nIn this setup:\n\nThe esm and cjs folders contain the ESM and CommonJS builds respectively.\n\nThe esm folder contains a package.json with the content { \"type\": \"module\" }.\n\nThe cjs folder contains a package.json with the content { \"type\": \"commonjs\" }.\n\nThis should cover most of the issues you might encounter when writing dual module libraries. Unfortunately, you may still run into some edge cases in more specialized setups, but hopefully, this post has given you a good starting point.\n"},{"id":"my-zsh-setup","title":"My Zsh Setup","description":"Zsh is super-customizable, and provides a great experience once configured properly. Here's how I set up my Zsh shell.","content":"Few years ago, I wrote Supercharge Your Terminal with Zsh. Since then, I've made a few changes to my Zsh setup. So it's time for an update.\n\nI won't go through the installation process, as it depends on your operating system. So make sure to install it first using your favorite package manager. If you're on MacOS, an older version of zsh is pre-installed, so you still may want to install a newer version with Homebrew or another package manager.\n\nTo make Zsh your default shell, run:\n\nNow we're ready to configure Zsh by editing the ~/.zshrc file.\n\nIf you want to see my full Zsh configuration, jump to the Wrapping Up section.\n\nWhy not Oh My Zsh?\n\nOne of the most popular Zsh frameworks is Oh My Zsh. It's an easy way to have a great Zsh setup with a lot of plugins and themes. However, it adds a lot of things that I don't need. All these additional code also slows down the shell startup time. So I set up my Zsh manually with the plugins and configurations that I need.\n\nPlugin manager\n\nWhen configuring Zsh, we often need to install \"plugins\". They are often installed using a plugin manager. By now I have switched the plugin manager many times since the ones I used before got abandoned. And now there are so many of them that it's hard to choose.\n\nYou can see a list of plugin managers and their benchmarks at rossmacarthur/zsh-plugin-manager-benchmark.\n\nPersonally I decided to not use a plugin manager and use a simple function that clones and sources the plugins:\n\nMake sure to place it at the top of your ~/.zshrc file so that the plugins are loaded before they are used. For a more advanced setup, see mattmc3/zsh\\_unplugged.\n\nYou can also choose to use a plugin manager if you prefer. You'll need to install it first, and adapt the code in this article to match the plugin manager you choose.\n\nAutocompletions\n\nTo setup autocompletion, we need to load compinit. However, compinit is the slowest part of my shell's startup. So with the below snippet, we use a cache for compinit if it was last updated a day ago:\n\nThe first part in updated\\_at in the above snippet is for Linux, and the second part is for MacOS.\n\nThen we need to load the complist module that provides a list of completions to select from:\n\nThen we can configure the style of the completions:\n\nWe can tweak few more things to improve the autocomplete menu:\n\nNow when we type a command and press Tab, we can cycle through the completions with the arrow keys and select one with Enter, instead of cycling through them with Tab.\n\nIn addition, we can also add the zsh-users/zsh-completions plugin to get additional completions. To use it, add it to the plugins array:\n\nSyntax Highlighting\n\nThe zdharma-continuum/fast-syntax-highlighting plugin can add syntax highlighting the commands as you type them. In addition, when typing a command, it’ll be highlighted in red if it’s invalid and in green if it’s valid.\n\nTo use it, add it to the plugins array:\n\nOne thing I disable is the paste highlighting. When I paste a command, the plugin adds a background to the pasted text to add a highlight. It makes it hard for me to see the cursor to edit the pasted text. So I disable it with the following:\n\nAutosuggestions\n\nThe zsh-users/zsh-autosuggestions plugin can suggest completions based on your command history. which you can select with the right arrow key (➡).\n\nTo set it up, first we need to configure Zsh to store the history in a file since it's not enabled by default:\n\nWe can also configure the history items to ignore duplicates and other improvements:\n\nThen we can add the plugin to the plugins array:\n\nThe plugin depends on the zdharma-continuum/fast-syntax-highlighting plugin, so make sure to add it to the plugins array as well.\n\nSubstring Search\n\nThe zsh-users/zsh-history-substring-search lets us type part of a command that exists in the history, and then select the matching command with a keybinding.\n\nTo use it, add it to the plugins array:\n\nThen we need to configure the keybindings. For example, to select with up and down arrow keys, we need to add the following configuration:\n\nFuzzy Search\n\nOne of the coolest tools I use is fzf. It's a command-line fuzzy finder that can be used to search through history, files, and more. I also like to integrate it with Zsh so my autocomplete menu is replaced with fzf.\n\nTo use it, first make sure to install it using your package manager. Then add the following to your ~/.zshrc file:\n\nNow when you typ Ctrl + R, it'll bring up a fuzzy search menu to search through the history, and when you press Ctrl + T, it'll bring up a fuzzy search menu to search through the files in the current directory.\n\nI also have a color scheme for fzf to match Palenight since it doesn't use the shell colors. You can set it using the FZF\\_DEFAULT\\_OPTS environment variable:\n\nNext step is to integrate it with the autocompletions. We can use the Aloxaf/fzf-tab plugin to achieve this. To use it, add it to the plugins array:\n\nThen we can configure the style of the completions:\n\nMake sure to remove the previous completion configuration (starting with zstyle ':completion) as it's not needed anymore.\n\nNow when you type a command and press Tab, you'll see a fuzzy search menu instead of the regular completion menu.\n\nCustom Prompt\n\nI use the spaceship prompt for my setup. It shows various information like the current directory, git status, and more. It loads information such as git status asynchronously by default, so the prompt is available immediately.\n\nSpaceship Prompt\n\nThis is my configuration:\n\nTo use it, add it to the plugins array:\n\nAnd then source the prompt (after the code that clones the plugins):\n\nIt's also necessary to install Nerd Fonts font so that icons in the prompt are displayed correctly. I use the FiraCode Nerd Font.\n\nI'm happy with the spaceship prompt as it loads quite fast, simple to customize, and provides the information I want. But if you're looking for something else, there are many other prompts available. Some of the popular ones are:\n\nOh My Posh\n\nStarship\n\nPowerlevel10k (in maintenance mode)\n\nMiscellaneous\n\nA cool feature in Zsh is the ability to navigate to a directory by typing the directory name without cd, or going up a directory with .. for 1 level, ../.. for 2 levels, and so on. To enable it, we can do the following:\n\nI make typos all the time. Zsh can autocorrect those typos and ask us to run the correct command when we try to run a wrong command. To enable it, we can do the following:\n\nSometimes I copy/paste content of a file to the terminal which may contain comments. By default it will result in a syntax error. To allow comments in interactive shells, we can do the following:\n\nIn some environments, the delete key doesn’t work as expected and inputs ~ instead. To workaround this, we need to add the following keybinding:\n\nCredits: https://blog.pilif.me/2004/10/21/delete-key-in-zsh.\n\nOn Terminal.app on Mac OS, opening a new tab doesn’t preserve the current working directory when using Zsh. To make it work, we need to add the following:\n\nThough now I use Ghostty which doesn't have this issue.\n\nWrapping Up\n\nTo make it easier to copy/paste, here is the complete configuration with all above tweaks:\n\nYou can copy the whole config and paste it to your ~/.zshrc file. The only pre-requisites are that you need to have git and fzf installed. It will automatically install the required plugins on the first run.\n\nMy current Zsh configuration is in my dotfiles repo. It also includes a few more tweaks that I didn't cover here.\n\nIf you are looking for more Zsh plugins, check out awesome-zsh-plugins. However, be mindful of how many plugins you install as they can slow down your shell startup time. Happy Zsh-ing! 🚀\n"}]